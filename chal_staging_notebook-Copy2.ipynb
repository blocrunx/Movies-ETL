{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "from config import db_password\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETL pipeline for wikipedia.movies.json, kaggle_metadat.csv, and ratings.csv\n",
    "\n",
    "def etl_wiki_kaggle_ratings(wikipedia_data, kaggle_data, movielens_ratings):\n",
    "    \n",
    "# Function to get json data and check to make sure there were no problems importing\n",
    "\n",
    "    try:\n",
    "        # Import wikipedia json data\n",
    "        with open (f'Resources/{wikipedia_data}', mode='r')as file:\n",
    "            wiki_movies_raw = json.load(file)\n",
    "        print(f\"Successfully imported {wikipedia_data} file.\")\n",
    "    \n",
    "    except Exception as e: print(e)\n",
    "    \n",
    "    # Confirm wiki file imported correctly then do some house cleaning on json file.\n",
    "    try:\n",
    "        if len(wiki_movies_raw) == 7311:\n",
    "            wiki_movies = [movie for movie in wiki_movies_raw\n",
    "            if ('Director' in movie or 'Directed by' in movie)\n",
    "            and 'imdb_link' in movie\n",
    "            and 'No. of episodes' not in movie]\n",
    "            print(f\"Wikipedia movies json data matches specified requirements.\")\n",
    "\n",
    "        else:\n",
    "            print(\"Error importing json data. Please try again.\")\n",
    "    \n",
    "    except Exception as e: print(e)\n",
    "    \n",
    "    # Import kaggle data\n",
    "    try:\n",
    "        file_path_kaggle_metadata = os.path.join('Resources', kaggle_data)\n",
    "        kaggle_metadata = pd.read_csv(file_path_kaggle_metadata, low_memory=False)\n",
    "        print(f\"Successfully imported Kaggle data.\")\n",
    "       \n",
    "    except Exception as e: print(e)\n",
    "        \n",
    "    # Import ratings data\n",
    "    try:\n",
    "        file_path_ratings = os.path.join('Resources', movielens_ratings)\n",
    "        ratings = pd.read_csv(file_path_ratings, low_memory=False)\n",
    "        print(f\"Successfully imported Ratings data.\")\n",
    "        \n",
    "    except Exception as e: print(e)\n",
    "    \n",
    "    # Clean movie data\n",
    "    def clean_movie(movie):\n",
    "        \n",
    "        try:\n",
    "            #remember movie is local and will not affect the global movie variable\n",
    "            movie = dict(movie)\n",
    "            alt_titles = {}\n",
    "            for key in ['Also known as','Arabic','Cantonese','Chinese','French',\n",
    "                        'Hangul','Hebrew','Hepburn','Japanese','Literally',\n",
    "                        'Mandarin','McCune–Reischauer','Original title','Polish',\n",
    "                        'Revised Romanization','Romanized','Russian',\n",
    "                        'Simplified','Traditional','Yiddish']:\n",
    "                \n",
    "                if key in movie:\n",
    "                    alt_titles[key] = movie[key]\n",
    "                    movie.pop(key)\n",
    "            \n",
    "            if len(alt_titles) > 0:\n",
    "                movie['alt_titles'] = alt_titles\n",
    "\n",
    "             # merge column names\n",
    "            def change_column_name(old_name, new_name):\n",
    "                if old_name in movie:\n",
    "                    movie[new_name] = movie.pop(old_name)\n",
    "            \n",
    "            change_column_name('Adaptation by', 'Writer(s)')\n",
    "            change_column_name('Country of origin', 'Country')\n",
    "            change_column_name('Directed by', 'Director')\n",
    "            change_column_name('Distributed by', 'Distributor')\n",
    "            change_column_name('Edited by', 'Editor(s)')\n",
    "            change_column_name('Length', 'Running time')\n",
    "            change_column_name('Original release', 'Release date')\n",
    "            change_column_name('Music by', 'Composer(s)')\n",
    "            change_column_name('Produced by', 'Producer(s)')\n",
    "            change_column_name('Producer', 'Producer(s)')\n",
    "            change_column_name('Productioncompanies ', 'Production company(s)')\n",
    "            change_column_name('Productioncompany ', 'Production company(s)')\n",
    "            change_column_name('Released', 'Release Date')\n",
    "            change_column_name('Release Date', 'Release date')\n",
    "            change_column_name('Screen story by', 'Writer(s)')\n",
    "            change_column_name('Screenplay by', 'Writer(s)')\n",
    "            change_column_name('Story by', 'Writer(s)')\n",
    "            change_column_name('Theme music composer', 'Composer(s)')\n",
    "            change_column_name('Written by', 'Writer(s)')            \n",
    "            \n",
    "            return(movie)\n",
    "        \n",
    "        except Exception as e: print(e)\n",
    "            \n",
    "    try:        \n",
    "        # Create pandas DataFrame\n",
    "        clean_movies = [clean_movie(movie) for movie in wiki_movies]\n",
    "        wiki_movies_df = pd.DataFrame(clean_movies)\n",
    "        \n",
    "        # Let the user know what's happening\n",
    "        print(\"Successfully created wiki_movies_df DataFrame.\")\n",
    "        \n",
    "    except Exception as e: print(e)\n",
    "    \n",
    "    try:\n",
    "        # Drop duplicate IMDB ID's\n",
    "        wiki_movies_df['imdb_id'] = wiki_movies_df['imdb_link'].str.extract(r'(tt\\d{7})')\n",
    "\n",
    "        wiki_movies_df.drop_duplicates(subset='imdb_id', inplace=True)\n",
    "\n",
    "\n",
    "        # Remove mostly null columns\n",
    "        wiki_columns_to_keep = [column for column in wiki_movies_df.columns if wiki_movies_df[column].isnull().sum() < len(wiki_movies_df) * 0.9]\n",
    "        wiki_movies_df = wiki_movies_df[wiki_columns_to_keep]\n",
    "        \n",
    "\n",
    "        # Clean box office column using regex\n",
    "        form_one = r'\\$\\s*\\d+\\.?\\d*\\s*[mb]illi?on'\n",
    "        form_two = r'\\$\\s*\\d{1,3}(?:[,\\.]\\d{3})+(?!\\s[mb]illion)'\n",
    "\n",
    "        # Drop na\n",
    "        box_office = wiki_movies_df['Box office'].dropna()\n",
    "\n",
    "        # Convert lists to strings\n",
    "        box_office = box_office.apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "\n",
    "        # Find values given as range\n",
    "        box_office = box_office.str.replace(r'\\$.*[-—–](?![a-z])', '$', regex=True)\n",
    "    \n",
    "    except Exception as e: print(e)\n",
    "    \n",
    "    def parse_dollars(s):\n",
    "        try:\n",
    "            # if s is not a string, return NaN\n",
    "            if type(s) != str:\n",
    "                return np.nan\n",
    "\n",
    "            # if input is of the form $###.# million\n",
    "            if re.match(r'\\$\\s*\\d+\\.?\\d*\\s*milli?on', s, flags=re.IGNORECASE):\n",
    "\n",
    "                # remove dollar sign and \" million\"\n",
    "                s = re.sub('\\$|\\s|[a-zA-Z]','', s)\n",
    "\n",
    "                # convert to float and multiply by a million\n",
    "                value = float(s) * 10**6\n",
    "\n",
    "                # return value\n",
    "                return value\n",
    "\n",
    "            # if input is of the form $###.# billion\n",
    "            elif re.match(r'\\$\\s*\\d+\\.?\\d*\\s*billi?on', s, flags=re.IGNORECASE):\n",
    "\n",
    "                # remove dollar sign and \" billion\"\n",
    "                s = re.sub('\\$|\\s|[a-zA-Z]','', s)\n",
    "\n",
    "                # convert to float and multiply by a billion\n",
    "                value = float(s) * 10**9\n",
    "\n",
    "                # return value\n",
    "                return value\n",
    "\n",
    "            # if input is of the form $###,###,###\n",
    "            elif re.match(r'\\$\\s*\\d{1,3}(?:[,\\.]\\d{3})+(?!\\s[mb]illion)', s, flags=re.IGNORECASE):\n",
    "\n",
    "                # remove dollar sign and commas\n",
    "                s = re.sub('\\$|,','', s)\n",
    "\n",
    "                # convert to float\n",
    "                value = float(s)\n",
    "\n",
    "                # return value\n",
    "                return value\n",
    "\n",
    "            # otherwise, return NaN\n",
    "            else:\n",
    "                return np.nan\n",
    "        \n",
    "        except Exception as e: print(e)\n",
    "            \n",
    "    try:        \n",
    "        # Change formatting for values in box office to new column 'box_0ffice' and drop old column 'Box office'\n",
    "        wiki_movies_df['box_office'] = box_office.str.extract(f'({form_one}|{form_two})', flags=re.IGNORECASE)[0].apply(parse_dollars)\n",
    "\n",
    "        wiki_movies_df.drop('Box office', axis=1, inplace=True)\n",
    "        #print(wiki_movies_df.columns.tolist())\n",
    "    \n",
    "    except Exception as e: print(e)\n",
    "    \n",
    "    # Clean wiki budget column\n",
    "    try:\n",
    "        # Drop na\n",
    "        budget = wiki_movies_df['Budget'].dropna()\n",
    "       \n",
    "        # Convert any lists to strings\n",
    "        budget = budget.map(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "        \n",
    "        # Remove any values between dollar sign and hyphen\n",
    "        budget = budget.str.replace(r'\\$.*[-—–](?![a-z])', '$', regex=True)\n",
    "        \n",
    "        # Remove citation references\n",
    "        budget = budget.str.replace(r'\\[\\d+\\]\\s*', '')\n",
    "        \n",
    "        # Assign values extrated with regex to new column\n",
    "        wiki_movies_df['budget'] = budget.str.extract(f'({form_one}|{form_two})', flags=re.IGNORECASE)[0].apply(parse_dollars)\n",
    "        \n",
    "        # Drop the original budget column\n",
    "        wiki_movies_df.drop('Budget', axis=1, inplace=True)\n",
    "    \n",
    "    except Exception as e: print(e)\n",
    "    \n",
    "    # Clean release date data:\n",
    "    try:\n",
    "        # Drop na values in release date column\n",
    "        release_date = wiki_movies_df['Release date'].dropna().apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "\n",
    "        # Apply regex forms\n",
    "        # Release date regex forms\n",
    "        date_form_one = r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s[123]\\d,\\s\\d{4}'\n",
    "        date_form_two = r'\\d{4}.[01]\\d.[123]\\d'\n",
    "        date_form_three = r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s\\d{4}'\n",
    "        date_form_four = r'\\d{4}'\n",
    "        \n",
    "        # Use forms and change datatype to datetime \n",
    "        wiki_movies_df['release_date'] = pd.to_datetime(release_date.str.extract(f'({date_form_one}|{date_form_two}|{date_form_three}|{date_form_four})')[0], infer_datetime_format=True)\n",
    "    \n",
    "    except Exception as e: print(e)\n",
    "        \n",
    "    try:\n",
    "        # Clean Running times data:\n",
    "        # Drop na\n",
    "        running_time = wiki_movies_df['Running time'].dropna().apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "        \n",
    "        # Apply regex\n",
    "        running_time_extract = running_time.str.extract(r'(\\d+)\\s*ho?u?r?s?\\s*(\\d*)|(\\d+)\\s*m')\n",
    "        \n",
    "        # Convert empty strings to nan with 'coerce' then to 0 with fillna(0)\n",
    "        running_time_extract = running_time_extract.apply(lambda col: pd.to_numeric(col, errors='coerce')).fillna(0)\n",
    "        \n",
    "        #function that will convert the hour capture groups and minute capture groups to minutes if the pure minutes capture group is zero\n",
    "        wiki_movies_df['running_time'] = running_time_extract.apply(lambda row: row[0]*60 + row[1] if row[2] == 0 else row[2], axis=1)\n",
    "        \n",
    "        # Drop Running time column\n",
    "        wiki_movies_df.drop('Running time', axis=1, inplace=True)\n",
    "    \n",
    "    except Exception as e: print(e)\n",
    "    \n",
    "    print(\"Successfully cleaned wiki_movies_df DataFrame.\")\n",
    "    \n",
    "    ### Kaggle Data ###\n",
    "    \n",
    "    try:\n",
    "        # Drop adult column from hackathon data\n",
    "        kaggle_metadata = kaggle_metadata[kaggle_metadata['adult'] == 'False'].drop('adult',axis='columns')\n",
    "        print(f\"Successfully dropped Kaggle 'adult' column.\")\n",
    "    \n",
    "    except Exception as e: print(e)\n",
    "        \n",
    "    try:\n",
    "        # create a column with boolean masks\n",
    "        kaggle_metadata['video'] = kaggle_metadata['video'] == 'True'\n",
    "        \n",
    "        # Change columns data type\n",
    "        kaggle_metadata['budget'] = kaggle_metadata['budget'].astype(int)\n",
    "        kaggle_metadata['id'] = pd.to_numeric(kaggle_metadata['id'], errors='raise')\n",
    "        kaggle_metadata['popularity'] = pd.to_numeric(kaggle_metadata['popularity'], errors='raise')\n",
    "        kaggle_metadata['release_date'] = pd.to_datetime(kaggle_metadata['release_date'])\n",
    "        print(f\"Successfully cleaned Kaggle data.\")\n",
    "    \n",
    "    except Exception as e: print(e)\n",
    "    \n",
    "    ### Ratings Data ###\n",
    "    \n",
    "    # Convert ratings timestamp to datetime\n",
    "    try:\n",
    "        ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], unit='s')\n",
    "    \n",
    "    except Exception as e: print(e)\n",
    "    \n",
    "    ### Merge wiki_movies_df and kaggle_metadata ###\n",
    "    \n",
    "    try:\n",
    "        movies_df = pd.merge(wiki_movies_df, kaggle_metadata, on='imdb_id', suffixes=['_wiki','_kaggle'])\n",
    "        print(f\"Successfully merged wiki_movies_df and kaggle_metadata.\")\n",
    "    \n",
    "    except Exception as e: print(e)\n",
    "        \n",
    "    try:\n",
    "        # Drop lofi dupplicate columns\n",
    "        movies_df.drop(columns=['title_wiki','release_date_wiki','Language','Production company(s)'], inplace=True)\n",
    "        \n",
    "        #  Fill missing kaggle data with wiki data\n",
    "        def fill_missing_kaggle_data(df, kaggle_column, wiki_column):\n",
    "            df[kaggle_column] = df.apply(lambda row: row[wiki_column] if row[kaggle_column] == 0 else row[kaggle_column], axis=1)\n",
    "            df.drop(columns=wiki_column, inplace=True)\n",
    "            \n",
    "        fill_missing_kaggle_data(movies_df, 'runtime', 'running_time')\n",
    "        fill_missing_kaggle_data(movies_df, 'budget_kaggle', 'budget_wiki')\n",
    "        fill_missing_kaggle_data(movies_df, 'revenue', 'box_office')\n",
    "        print(f\"Successfully filled missing Kaggle data with wiki data.\")\n",
    "        \n",
    "    except Exception as e: print(e)\n",
    "    \n",
    "    try:\n",
    "        # Reorder columns\n",
    "        movies_df = movies_df[['imdb_id','id','title_kaggle','original_title','tagline','belongs_to_collection','url','imdb_link',\n",
    "                           'runtime','budget_kaggle','revenue','release_date_kaggle','popularity','vote_average','vote_count',\n",
    "                           'genres','original_language','overview','spoken_languages','Country',\n",
    "                           'production_companies','production_countries','Distributor',\n",
    "                           'Producer(s)','Director','Starring','Cinematography','Editor(s)','Writer(s)','Composer(s)','Based on'\n",
    "                          ]]\n",
    "        print(f\"Successfully reordered columns in movies_df.\")\n",
    "    \n",
    "    except Exception as e: print(e)\n",
    "    \n",
    "    try:\n",
    "        # Rename columns\n",
    "        movies_df.rename({'id':'kaggle_id',\n",
    "                  'title_kaggle':'title',\n",
    "                  'url':'wikipedia_url',\n",
    "                  'budget_kaggle':'budget',\n",
    "                  'release_date_kaggle':'release_date',\n",
    "                  'Country':'country',\n",
    "                  'Distributor':'distributor',\n",
    "                  'Producer(s)':'producers',\n",
    "                  'Director':'director',\n",
    "                  'Starring':'starring',\n",
    "                  'Cinematography':'cinematography',\n",
    "                  'Editor(s)':'editors',\n",
    "                  'Writer(s)':'writers',\n",
    "                  'Composer(s)':'composers',\n",
    "                  'Based on':'based_on'\n",
    "                 }, axis='columns', inplace=True)\n",
    "        print(f\"Successfully renamed columns.\")\n",
    "    \n",
    "    except Exception as e: print(e)\n",
    "    \n",
    "    ### Transform Rating Data and merge with movies_df ###\n",
    "    try:\n",
    "        # pivot data so movieId is the index, rating values are the columns and rows are the count for each rating value.\n",
    "        rating_counts = ratings.groupby(['movieId','rating'], as_index=False).count().rename({'userId':'count'}, axis=1).pivot(index='movieId',columns='rating', values='count')\n",
    "        \n",
    "        # Rename columns with list comprehension.\n",
    "        rating_counts.columns = ['rating_' + str(col) for col in rating_counts.columns]\n",
    "        \n",
    "        # Merge data with movies_df.\n",
    "        movies_with_ratings_df = pd.merge(movies_df, rating_counts, left_on='kaggle_id', right_index=True, how='left')\n",
    "        \n",
    "        # Fill na values with 0.\n",
    "        movies_with_ratings_df[rating_counts.columns] = movies_with_ratings_df[rating_counts.columns].fillna(0)\n",
    "        \n",
    "        print(f\"Successfully merged ratings data and movies_df\")\n",
    "    \n",
    "    except Exception as e: print(e)\n",
    "        \n",
    " ### Connect to database ###\n",
    "    \n",
    "    try:\n",
    "        # Create db engine and connect to database\n",
    "        db_string = f\"postgres://postgres:{db_password}@127.0.0.1:5432/movie_data\"\n",
    "        engine = create_engine(db_string)\n",
    "        print(f\"Successfully created database engine.\")\n",
    "    \n",
    "    except Exception as e: print(e)\n",
    "    \n",
    "    try:\n",
    "        # Import movies info to postgres\n",
    "        movies_df.to_sql(name='movies', con=engine, if_exists='replace')\n",
    "        print(f\"Successfully imported movies data to Postgresql\")\n",
    "    except Exception as e: print(e)\n",
    "        \n",
    "        \n",
    "    try:\n",
    "        # Import ratings to Postgres\n",
    "        rows_imported = 0\n",
    "        \n",
    "        # get the start_time from time.time()\n",
    "        start_time = time.time()\n",
    "        \n",
    "        \n",
    "        for data in pd.read_csv(f'Resources/ratings.csv', chunksize=1000000):\n",
    "            \n",
    "             # print out the range of rows that are being imported\n",
    "            print(f'importing rows {rows_imported} to {rows_imported + len(data)}...', end='')\n",
    "            \n",
    "            if rows_imported == 0 :\n",
    "                data.to_sql(name='ratings', con=engine, if_exists='replace')\n",
    "            else:\n",
    "                data.to_sql(name='ratings', con=engine, if_exists='append')\n",
    "            \n",
    "            rows_imported += len(data)\n",
    "\n",
    "            # add elapsed time to final print out\n",
    "            print(f'Done. {time.time() - start_time} total seconds elapsed')\n",
    "        \n",
    "\n",
    "    except Exception as e: print(e)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported wikipedia.movies.json file.\n",
      "Wikipedia movies json data matches specified requirements.\n",
      "Successfully imported Kaggle data.\n",
      "Successfully imported Ratings data.\n",
      "Successfully created wiki_movies_df DataFrame.\n",
      "Successfully cleaned wiki_movies_df DataFrame.\n",
      "Successfully dropped Kaggle 'adult' column.\n",
      "Successfully cleaned Kaggle data.\n",
      "Successfully merged wiki_movies_df and kaggle_metadata.\n",
      "Successfully filled missing Kaggle data with wiki data.\n",
      "Successfully reordered columns in movies_df.\n",
      "Successfully renamed columns.\n",
      "Successfully merged ratings data and movies_df\n",
      "Successfully created database engine.\n",
      "Successfully imported movies data to Postgresql\n",
      "importing rows 0 to 1000000...Done. 99.05202889442444 total seconds elapsed\n",
      "importing rows 1000000 to 2000000...Done. 195.93244671821594 total seconds elapsed\n",
      "importing rows 2000000 to 3000000...Done. 293.43077754974365 total seconds elapsed\n",
      "importing rows 3000000 to 4000000...Done. 394.78075647354126 total seconds elapsed\n",
      "importing rows 4000000 to 5000000...Done. 492.4991009235382 total seconds elapsed\n",
      "importing rows 5000000 to 6000000...Done. 590.6957623958588 total seconds elapsed\n",
      "importing rows 6000000 to 7000000...Done. 689.5261838436127 total seconds elapsed\n",
      "importing rows 7000000 to 8000000...Done. 787.2105286121368 total seconds elapsed\n",
      "importing rows 8000000 to 9000000...Done. 885.3719053268433 total seconds elapsed\n",
      "importing rows 9000000 to 10000000...Done. 984.3723464012146 total seconds elapsed\n",
      "importing rows 10000000 to 11000000...Done. 1083.0037591457367 total seconds elapsed\n",
      "importing rows 11000000 to 12000000...Done. 1184.1953654289246 total seconds elapsed\n",
      "importing rows 12000000 to 13000000...Done. 1283.24280834198 total seconds elapsed\n",
      "importing rows 13000000 to 14000000...Done. 1388.6957347393036 total seconds elapsed\n",
      "importing rows 14000000 to 15000000...Done. 1486.2770690917969 total seconds elapsed\n",
      "importing rows 15000000 to 16000000...Done. 1587.300668001175 total seconds elapsed\n",
      "importing rows 16000000 to 17000000...Done. 1685.1050128936768 total seconds elapsed\n",
      "importing rows 17000000 to 18000000...Done. 1784.6620104312897 total seconds elapsed\n",
      "importing rows 18000000 to 19000000...Done. 1883.9434757232666 total seconds elapsed\n",
      "importing rows 19000000 to 20000000..."
     ]
    }
   ],
   "source": [
    "etl_wiki_kaggle_ratings('wikipedia.movies.json', 'movies_metadata.csv', 'ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
